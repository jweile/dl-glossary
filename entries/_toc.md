## Table of contents

 * [Activation function](#activation-function)
 * [Affine layer](#affine-layer)
 * [Attention mechanism](#attention-mechanism)
 * [Autoencoder](#autoencoder)
 * [Average-Pooling](#average-pooling)
 * [Backpropagation](#backpropagation)
 * [Backward pass](#backward-pass)
 * [Batch normalization](#batch-normalization)
 * [Batch](#batch)
 * [Bias](#bias)
 * [Capsule Network](#capsule-network)
 * [Convolutional Neural Network (CNN)](#cnn)
 * [Context](#context)
 * [Data augmentation](#data-augmentation)
 * [Dead neuron](#dead-neuron)
 * [Decoder](#decoder)
 * [Dropout](#dropout)
 * [Embedding](#embedding)
 * [Encoder](#encoder)
 * [Epoch](#epoch)
 * [Exploding gradient](#exploding-gradient)
 * [Feed-forward](#feed-forward)
 * [Fine-tuning](#fine-tuning)
 * [Forward pass](#forward-pass)
 * [Generative Adversarial Network (GAN)](#gan)
 * [Graph Convolutional Network (GCN)](#gcn)
 * [Gradient Recurrent Unit (GRU)](#gru)
 * [Kernel](#kernel)
 * [Latent space](#latent-space)
 * [Layer](#layer)
 * [Learning rate](#learning-rate)
 * [Loss function](#loss-function)
 * [Long Short-Term Memory (LSTM)](#lstm)
 * [Max-Pooling](#max-pooling)
 * [Multi Layer Perceptron (MLP)](#mlp)
 * [Module](#module)
 * [Neuron](#neuron)
 * [Normalization layer](#normalization-layer)
 * [Pooling](#pooling)
 * [Pytorch](#pytorch)
 * [RAG](#rag)
 * [Receptive field](#receptive-field)
 * [Relational reasoning](#relational-reasoning)
 * [ReLU](#relu)
 * [Residual Networks (ResNet)](#resnet)
 * [Recurrent Neural Network (RNN)](#rnn)
 * [Sequence-to-sequence (S2S)](#s2s)
 * [Siamese Neural Network](#siamese-neural-network)
 * [Softmax](#softmax)
 * [Tensorflow](#tensorflow)
 * [Training](#training)
 * [Vanishing gradient](#vanishing-gradient)
