## Exploding gradient
The Exploding Gradient Problem is the opposite of the [Vanishing Gradient Problem](#vanishing-gradient). In Deep Neural Networks gradients may explode during backpropagation, resulting number overflows. A common technique to deal with exploding gradients is to perform Gradient Clipping or using LeakyReLU activation function.

* [On the difficulty of training recurrent neural networks](http://arxiv.org/abs/1211.5063)

