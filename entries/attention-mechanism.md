## Attention mechanism

Attention Mechanisms are inspired by human attention, the ability to focus on specific parts of a larger context. Attention is the core context of the [Transformer](#transformer), in which specific features "attend to" other features. Mathematically, this is achieved by multiplying the features with "query" and "key" vectors. The 

* [Attention and Memory in Deep Learning and NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)

