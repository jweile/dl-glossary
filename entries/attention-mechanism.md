## Attention mechanism

Attention Mechanisms are inspired by human attention, the ability to focus on specific parts of a larger context. Attention is the core context of the [Transformer](#transformer), in which specific features "attend to" other features. 

`TODO: write more here`

* [Attention and Memory in Deep Learning and NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)

