## Activation function

Activation functions live inside neural network layers and modify the data they receive before passing it to the next layer. Some of them mimic the non-linear behavior of biological neurons, introducing a saturation behavior and an activation threshold. 

The choice of activation function can contribute to problems such as "[dead neurons](#dead-neuron)" as well as compute time for [forward-](forward-pass) and [backward-passes](backward-pass).

Commonly used functions include [tanh](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#tanh), [ReLU (Rectified Linear Unit)](#relu), [sigmoid](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html#sigmoid), and variants of these.

